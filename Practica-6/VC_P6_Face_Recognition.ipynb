{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 6\n",
    "## Propuesta: Intento de sistema de reconocimiento facial y reacciones según datos de DeepFace\n",
    "<ul>\n",
    "<li>Nikhil Chandru Durgadas Chellaram</li>\n",
    "<li>Raúl Mateus Sánchez</li>\n",
    "</ul>\n",
    "\n",
    "Para comprobar el desarrollo de la práctica: https://github.com/raulmat19/Vision-por-Computador/tree/main/Practica-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"VGG-Face\",\"Facenet\",\"Facenet512\",\"OpenFace\",\"DeepFace\",\"DeepID\",\"ArcFace\",\"Dlib\",\"SFace\"]\n",
    "\n",
    "metrics = [\"cosine\", \"euclidean\", \"euclidean_l2\"]\n",
    "\n",
    "backends = ['opencv','ssd','dlib','mtcnn','retinaface','mediapipe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(img, text,\n",
    "          font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "          pos=(0, 0),\n",
    "          font_scale=3,\n",
    "          font_thickness=2,\n",
    "          text_color=(255, 255, 255),\n",
    "          text_color_bg=(0, 0, 0)\n",
    "          ):\n",
    "\n",
    "    x, y = pos\n",
    "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "    text_w, text_h = text_size\n",
    "    cv2.rectangle(img, pos, (x + text_w + 5, y + text_h + 5), text_color_bg, -1)\n",
    "    cv2.putText(img, text, (x, int(y + text_h + font_scale - 1)), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    return text_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este pequeño sistema, se permite al usuario registrar su cara en el sistema con la finalidad de simular un sistema de reconocimiento facial así como prodecer a las reacciones propuestas empleando las utilidades de DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFace(path, name):\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    counter = 0\n",
    "\n",
    "    while(True):      \n",
    "        # fotograma a fotograma\n",
    "        ret, frame = vid.read()\n",
    "\n",
    "        if ret:\n",
    "            draw_text(frame, f\"{counter}\", font_scale=1, pos=(50, 10), text_color_bg=(0, 0, 0))\n",
    "            cv2.imshow('Cam', frame)   \n",
    "\n",
    "        key = cv2.waitKey(5)     \n",
    "        # Detenemos pulsado ESC\n",
    "        if key and key == 27 or counter == 3:\n",
    "            break\n",
    "        elif key and key == 32:\n",
    "            if counter > 0:\n",
    "                new_path = path + '/' + name + str(counter) + '.jpg'\n",
    "            \n",
    "            else:\n",
    "                new_path = path + '/' + name + '.jpg'\n",
    "\n",
    "            cv2.imwrite(new_path, frame)\n",
    "            counter += 1\n",
    "            \n",
    "            \n",
    "    # Libera el objeto de captura\n",
    "    vid.release()\n",
    "    # Destruye ventanas\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_loaded = False\n",
    "\n",
    "data_loaded = input (\"Si su cara ha sido ya registrada, introduzca 'Si'. En caso contrario, introduzca 'No' o cualquier cosa\")\n",
    "if data_loaded == 'Si' or data_loaded == 'si':\n",
    "    face_loaded = True\n",
    "\n",
    "if not face_loaded:\n",
    "    data = False\n",
    "    escape = 0\n",
    "\n",
    "    while not data:\n",
    "\n",
    "        name = input (\"Introduce tu nombre: \")\n",
    "        surname = input (\"Introduce tus apellidos: \")\n",
    "        age = input (\"Introduce tu edad: \")\n",
    "        sexo = input (\"Introduce tu sexo: \")\n",
    "        workingStatus = input (\"Introduce tu ocupación: \")\n",
    "\n",
    "        if name == '' or surname == '' or age == '' or sexo == '' or workingStatus == '':\n",
    "            warning = input (\"Datos incorrectos. Pulse Enter para volver a introducirlos\")\n",
    "            escape += 1\n",
    "        \n",
    "        else:\n",
    "            data = True\n",
    "\n",
    "        if escape >= 2:\n",
    "            break\n",
    "\n",
    "    if data:\n",
    "        try:\n",
    "            for file in os.listdir(\"face_recognition_images/\"):\n",
    "                if file.startswith(\"representations\"):\n",
    "                    os.remove(\"face_recognition_images/\" + file)\n",
    "                    \n",
    "            dir_path = 'face_recognition_images/' + str(name)\n",
    "            try:\n",
    "                os.stat(dir_path)\n",
    "            except:\n",
    "                os.mkdir(dir_path)\n",
    "            file_path = dir_path + '/info.txt'\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(f\"{name}, {surname}, {age}, {sexo}, {workingStatus}\")\n",
    "            f.close()\n",
    "\n",
    "            warning = input (\"Pulse Enter para escanear su cara. Debe sacarse tres fotos, empleando para ello la tecla 'Barra espaciadora'\")\n",
    "            getFace(dir_path, name)\n",
    "\n",
    "        except:\n",
    "            print(\"Fallo del sistema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sistema de reconocimiento facial, que compara la cara en tiempo real con las caras ya escaneadas y guardadas, y muestra la información del usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/60674501/how-to-make-black-background-in-cv2-puttext-with-python-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFlag(ruta, img):\n",
    "    img_flag = cv2.imread(ruta)\n",
    "    img_flag = cv2.resize(img_flag, (150, 100))\n",
    "    draw_text(img, \"En el Mundial apoyas a: \", font_scale=1, pos=(50, img.shape[0]-50), text_color_bg=(0, 0, 0))\n",
    "    img[img.shape[0]-100:img.shape[0], img.shape[1]-150:img.shape[1]] = img_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFlag(raza):\n",
    "    if raza == \"white\":\n",
    "        return \"flags_images/white\"\n",
    "    elif raza == \"asian\":\n",
    "        return \"flags_images/asian\"\n",
    "    elif raza == \"indian\":\n",
    "        return \"flags_images/indian\"\n",
    "    elif raza == \"middle eastern\":\n",
    "        return \"flags_images/middle-eastern\"\n",
    "    elif raza == \"latino hispanic\":\n",
    "        return \"flags_images/latino-hispanic\"\n",
    "    elif raza == \"black\":\n",
    "        return \"flags_images/black\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Analyzing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "Analyzing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\n",
      "Analyzing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "Analyzing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n",
      "Analyzing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|██████████| 1/1 [00:00<00:00,  5.84it/s]\n",
      "Analyzing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n",
      "Analyzing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing: 100%|██████████| 1/1 [00:00<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: age:   0%|          | 0/4 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 409ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  25%|██▌       | 1/4 [00:00<00:01,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 398ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race:  50%|█████     | 2/4 [00:00<00:00,  2.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 411ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion:  75%|███████▌  | 3/4 [00:01<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 103ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 4/4 [00:01<00:00,  2.40it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ruta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m flag_list \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(path_to_flag)\n\u001b[0;32m     60\u001b[0m index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(flag_list))\n\u001b[1;32m---> 61\u001b[0m path_to_flag \u001b[39m=\u001b[39m ruta \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m flag_list[index]\n\u001b[0;32m     62\u001b[0m showFlag(path_to_flag, frame)\n\u001b[0;32m     63\u001b[0m prev_race_path \u001b[39m=\u001b[39m path_to_flag\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ruta' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl Kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. Revise el código de las celdas para identificar una posible causa del error. Haga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. Vea el [registro] de Jupyter (command:jupyter.viewOutput) para obtener más detalles."
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "vid = cv2.VideoCapture(0)\n",
    "counter = 0\n",
    "init = False\n",
    "flag_mode = False\n",
    "line = None\n",
    "y0, dy = 10, 4\n",
    "properties = [\"Nombre\", \"Apellidos\", \"Edad\", \"Sexo\", \"Ocupacion\"]\n",
    "prev_race_path = None\n",
    "prev_race = None\n",
    "\n",
    "while(True):      \n",
    "    # fotograma a fotograma\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    if ret:\n",
    "        # Sistema de reconocimiento facial y muestra de información de usuario\n",
    "        if not flag_mode:\n",
    "            if counter == 50 or not init:\n",
    "                cv2.imwrite('cap_faces/face.jpg', frame)\n",
    "\n",
    "                try:\n",
    "                    df = DeepFace.find(img_path = 'cap_faces/face.jpg', db_path = \"face_recognition_images/\", model_name=models[2], distance_metric = metrics[2], prog_bar = False, enforce_detection=False, silent = True)\n",
    "                    path = df.at[0, 'identity']\n",
    "                    info_path = os.path.dirname(path) + \"/info.txt\"\n",
    "                    with open(info_path) as f:\n",
    "                        line = f.readline()\n",
    "                    f.close()\n",
    "\n",
    "                    draw_text(frame, \"Desbloqueado\", font_scale=1, pos=(0, frame.shape[0]-50), text_color_bg=(0, 0, 0))\n",
    "                    for i, line in enumerate(line.split(\", \")):\n",
    "                        y = y0 + i*dy*7\n",
    "                        draw_text(frame, f\"{properties[i]}:  {line}\", font_scale=1, pos=(5, y), text_color_bg=(0, 0, 0))\n",
    "                    \n",
    "                    counter == 0\n",
    "                    init = True\n",
    "\n",
    "                except:\n",
    "                    init = False\n",
    "                    print(\"Not recognised\")\n",
    "                    draw_text(frame, \"Bloqueado\", font_scale=1, pos=(0, frame.shape[0]-50), text_color_bg=(0, 0, 0))\n",
    "\n",
    "            elif init and counter != 50:\n",
    "                \n",
    "                draw_text(frame, \"Desbloqueado\", font_scale=1, pos=(0, frame.shape[0]-50), text_color_bg=(0, 0, 0))\n",
    "                for i, line in enumerate(line.split(\", \")):\n",
    "                    y = y0 + i*dy*7\n",
    "                    draw_text(frame, f\"{properties[i]}:  {line}\", font_scale=1, pos=(5, y), text_color_bg=(0, 0, 0))\n",
    "                \n",
    "                counter += 1\n",
    "\n",
    "        elif flag_mode and init:\n",
    "            print(\"a\")\n",
    "            cv2.imwrite('cap_faces/frame.jpg', frame)\n",
    "            obj = DeepFace.analyze(img_path = 'cap_faces/frame.jpg', actions = ['age', 'gender', 'race', 'emotion'], enforce_detection=False)\n",
    "            race = obj['dominant_race']\n",
    "            if race != prev_race:\n",
    "                path_to_flag = getFlag(race)\n",
    "                flag_list = os.listdir(path_to_flag)\n",
    "                index = np.random.randint(0, len(flag_list))\n",
    "                path_to_flag = path_to_flag + \"/\" + flag_list[index]\n",
    "                showFlag(path_to_flag, frame)\n",
    "                prev_race_path = path_to_flag\n",
    "                prev_race = race\n",
    "\n",
    "            else:\n",
    "                showFlag(prev_race_path, frame)\n",
    "\n",
    "            draw_text(frame, f\"Raza: {race}\", font_scale=1, pos=(0, frame.shape[0]-470), text_color_bg=(0, 0, 0))\n",
    "\n",
    "        elif flag_mode and not init:\n",
    "            draw_text(frame, \"Accion bloqueada hasta identificacion.\", font_scale=1, pos=(0, frame.shape[0]-80), text_color_bg=(0, 0, 0))\n",
    "            draw_text(frame, \"Pulse d para ello\", font_scale=1, pos=(0, frame.shape[0]-50), text_color_bg=(0, 0, 0))\n",
    "        cv2.imshow('Cam', frame)\n",
    "                    \n",
    "    # Detenemos pulsado ESC\n",
    "    key = cv2.waitKey(5)\n",
    "    if key == 27:\n",
    "        break\n",
    "    if key == ord('d'):\n",
    "        if flag_mode:\n",
    "            flag_mode = False\n",
    "            init = False\n",
    "        else:\n",
    "            flag_mode = True\n",
    "  \n",
    "# Libera el objeto de captura\n",
    "vid.release()\n",
    "# Destruye ventanas\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En desarrollo ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('deepface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a45fc514aa0ff02d4189d20bd2042257a18249c899466a0c8f50819ba97afb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
